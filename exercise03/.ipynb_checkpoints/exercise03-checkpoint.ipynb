{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619cd606",
   "metadata": {},
   "source": [
    "### Exercise 3 - Dynamic Programming and LQR\n",
    "\n",
    "In this exercise, we will use dynamic programming (DP) to implement a controller for an inverted pendulum and compare it to a LQR control approach.\n",
    "\n",
    "The pendulum position is described by the angle $\\theta$ with corresponding angle velocity $\\omega$ and we can control it by applying a torque $\\tau$. This system can be described by idealized dynamics\n",
    "\\begin{equation}\\label{eq:dynamics}\n",
    "\\begin{aligned}\n",
    "\\dot{\\theta} &= \\omega, \\\\\n",
    "\\dot{\\omega} &= \\sin(\\theta) + \\tau,\n",
    "\\end{aligned}\n",
    "\\end{equation} \n",
    "where for simplicity we are ignoring all units. Defining the state $s=(\\theta, \\omega)$, with control actions $a=\\tau$, our control aim is to swing up the pendulum to the upright position and balancing it there, corresponding to a state of $s_\\mathrm{ref}=(0,0)$. We start with the pendulum hanging down in a rest position, i.e., $\\bar s_0 = (\\pi, 0)$. We can express this as the discrete time optimal control problem:\n",
    "<!-- \\begin{mini}|s|[2]\n",
    "{\\scriptstyle{\\substack{s_0, \\dots, s_N, \\\\ a_0, \\dots, a_{N-1}}}}\n",
    "{\n",
    "\t\\sum_{i=0}^{N-1}\n",
    "\t\\left(\\,s_i^TQs_i + a_i^TRa_i\\right)\n",
    "\t+ \n",
    "\ts_N^TQ_Ns_N,\n",
    "}\n",
    "{\\label{eq:nlp}}\n",
    "{}\n",
    "\\addConstraint{s_0}{= \\bar{s}_0}\n",
    "\\addConstraint{s_{i+1}}{= F(s_i, a_i),}{~i = 0, \\ldots, N-1}\n",
    "\\addConstraint{-10}{\\leq a_i \\leq 10,}{~i = 0, \\ldots, N-1}\n",
    "\\addConstraint{-\\frac{\\pi}{2}}{\\leq \\theta_i \\leq 2\\pi,}{~i = 0, \\ldots, N-1}\n",
    "\\addConstraint{-8}{\\leq \\omega_i \\leq 8,}{~i = 0, \\ldots, N-1,}\n",
    "\\end{mini} -->\n",
    "<img src=\"imgs/nlp.png\" alt=\"nlp\" width=\"350\"/>\n",
    "where $F(s,a)$ describes the discretized dynamics obtained by applying one step of the explicit RK4 integrator with step-size $h=0.1$ to the continuous time system dynamics.\n",
    "\n",
    "We choose $Q = \\textrm{diag}(100,\\, 0.01)$ and $R = 0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b966d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from casadi import SX, vertcat, Function, sin\n",
    "from model import Model\n",
    "from utils import integrate_RK4\n",
    "\n",
    "# continuous dynamics\n",
    "ns = 2   # dimension of the state\n",
    "na = 1   # dimension of the controls\n",
    "\n",
    "theta = SX.sym('theta')\n",
    "omega = SX.sym('omega')\n",
    "a = SX.sym('a')\n",
    "\n",
    "theta_dot = omega\n",
    "omega_dot = sin(theta) + a\n",
    "\n",
    "s = vertcat(theta, omega)\n",
    "s_dot = vertcat(theta_dot, omega_dot)\n",
    "\n",
    "# discrete dynamics\n",
    "dt = 0.1\n",
    "n_steps = 5\n",
    "F_discrete = integrate_RK4(s, a, s_dot, dt, n_steps)\n",
    "\n",
    "# steady state\n",
    "s_steady_state = np.zeros((ns, 1))\n",
    "a_steady_state = np.zeros((na, 1))\n",
    "\n",
    "model = Model(s, a, F_discrete, s_steady_state, a_steady_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354bd38c",
   "metadata": {},
   "source": [
    "### 3.1 LQR\n",
    "\n",
    "Consider the unconstrained linear quadratic infinite horizon problem that is obtained from the above nonlinear optimization problem by linearizing the dynamics at $s_\\mathrm{lin} = (0, 0)$, $a_\\mathrm{lin} = 0$, and dropping the control constraints,\n",
    "<!-- \\begin{mini}|s|[2]\n",
    "\t{\\scriptstyle{\\substack{s_0, s_1, \\dots, \\\\ a_0, a_1, \\dots}}}\n",
    "\t{\n",
    "\t\t\\sum_{i=0}^{\\infty}\n",
    "\t\t\\left(\\,s_i^TQs_i + a_i^TRa_i\\right)\n",
    "\t}\n",
    "\t{\\label{eq:LQRqp}}\n",
    "\t{}\n",
    "\t\\addConstraint{s_0}{= \\bar{s}_0}\n",
    "\t\\addConstraint{s_{i+1}}{= A s_i + B a_i,}{~i = 0, 1, \\ldots, }\n",
    "\\end{mini} -->\n",
    "<img src=\"imgs/qp.png\" alt=\"nlp\" width=\"280\"/>\n",
    "\n",
    "where $A:=\\frac{\\partial F(s,a)}{\\partial s}\\big\\rvert_{\\substack{s=s_\\mathrm{lin} \\\\ a = a_\\mathrm{lin}}}$ and $B:=\\frac{\\partial F(s,a)}{\\partial a}\\big\\rvert_{\\substack{s=s_\\mathrm{lin} \\\\ a = a_\\mathrm{lin}}}$.\n",
    "\n",
    "\n",
    "Complete the following template to obtain the LQR gain matrix $K$, which defines the optimal control at each stage as the time-independent linear feedback law $a^*_i(s) = -K s$ and the solution to the associated Riccati equation $P$.\n",
    "\n",
    "*Hint: Check out* `scipy.linalg.solve_discrete_are`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac884ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve_discrete_are\n",
    "\n",
    "def get_LQR_gain(A, B, Q, R):\n",
    "\n",
    "# FILL IN YOUR CODE HERE\n",
    "#     P = ...\n",
    "#     K = ...\n",
    "    return (K, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e95f14",
   "metadata": {},
   "source": [
    "Use $P$ and $K$ to compute the LQR cost and control policy for all $s$ within the state grid. Discretize the angle $\\theta$ into 101 values between $-\\frac{\\pi}{2}$ and $2\\pi$. Analogously, discretize the angular velocity into 51 values between -8 and 8 and the torque $\\tau$ into 21 values between 10 and -10. \n",
    "\n",
    "Don't forget to clip the controls to the given control bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize the state and control space\n",
    "s_min = [-np.pi/2, -8]\n",
    "s_max = [2*np.pi, 8]\n",
    "\n",
    "a_max = 10\n",
    "a_min = -a_max\n",
    "\n",
    "N_s1 = 101\n",
    "N_s2 = 51\n",
    "N_a = 21\n",
    "\n",
    "s1_grid = np.linspace(s_min[0], s_max[0], N_s1)\n",
    "s2_grid = np.linspace(s_min[1], s_max[1], N_s2)\n",
    "a_grid = np.linspace(a_min, a_max, N_a)\n",
    "\n",
    "#  mesh for plotting\n",
    "[S1, S2] = np.meshgrid(s1_grid, s2_grid)\n",
    "S1 = S1.T\n",
    "S2 = S2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf581ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup LQR: linearize at the steady state\n",
    "# FILL IN YOUR CODE HERE\n",
    "# HINT: check out the provided model class\n",
    "# A = ...\n",
    "# B = ...\n",
    "\n",
    "# weighting matrices\n",
    "Q = np.diag([100, 0.01])\n",
    "R = np.diag([0.001])\n",
    "\n",
    "(K, P) = get_LQR_gain(A, B, Q, R)\n",
    "\n",
    "LQR_cost = np.zeros(S1.shape)  # grid with cost of every state combination\n",
    "LQR_a = np.zeros(S1.shape)     # LQR control for every state combination\n",
    "\n",
    "# loop through all state and input combinations\n",
    "for i in range(S1.shape[0]):\n",
    "    for j in range(S1.shape[1]):\n",
    "        s = np.reshape(np.array([S1[i, j], S2[i, j]]), (ns, 1))\n",
    "\n",
    "# FILL IN YOUR CODE HERE\n",
    "#         LQR_cost[i,j] = ...\n",
    "#         LQR_a[i,j] = ...\n",
    "\n",
    "# FILL IN YOUR CODE HERE\n",
    "# clipping\n",
    "# LQR_a = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816da484",
   "metadata": {},
   "source": [
    "### 3.2 Dynamic Programming \n",
    "\n",
    "Complete the following template to implement the DP algorithm and use it to compute the cost-to-go associated with the initial state of the optimal control problem. Choose $N=20$, and use again $Q = \\textrm{diag}(100,\\, 0.01)$, $R = 0.001$ and $Q_N$ equal to the cost matrix associated with the LQR controller. We use the same discretization grid as for the LQR.\n",
    "\n",
    "*Remark: In order to compute the cost-to-go we project the state obtained by simulating the dynamics forward onto the defined discretization grid and interpolate the cost-to-go value.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import project_indices\n",
    "N_steps = 20\n",
    "\n",
    "# precompute discrete simulation map\n",
    "s_next_map = np.zeros((N_s1, N_s2, N_a, ns))\n",
    "\n",
    "# precompute the projected indices\n",
    "projected_indices_s1 = np.zeros((N_s1, N_s2, N_a, 2), dtype=np.int16)\n",
    "projected_indices_s2 = np.zeros((N_s1, N_s2, N_a, 2), dtype=np.int16)\n",
    "\n",
    "# loop through all state and input combinations\n",
    "for i in range(S1.shape[0]):\n",
    "    for j in range(S1.shape[1]):\n",
    "        s = np.reshape(np.array([S1[i, j], S2[i, j]]), (ns, 1))\n",
    "        \n",
    "        # compute next state for all u\n",
    "        for k in range(N_a):\n",
    "            s_next = np.reshape(model.simulate(s, a_grid[k]), (ns,))\n",
    "            s_next_map[i, j, k, :] = s_next\n",
    "\n",
    "            # project s_next to the state grid\n",
    "            projected_indices_s1[i,j,k, :] = project_indices(s_next[0], s1_grid)\n",
    "            projected_indices_s2[i,j,k, :] = project_indices(s_next[1], s2_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "from utils import interpolate_bilinear\n",
    "\n",
    "# initialization of cost-to-go function for all iterations with\n",
    "# infinity until we know a better cost\n",
    "dp_cost_to_go = inf*np.ones((S1.shape[0], S1.shape[1], N_steps+1))\n",
    "\n",
    "# terminal cost is same as LQR\n",
    "# FILL IN YOUR CODE HERE\n",
    "# dp_cost_to_go[:, :, 0] = ...\n",
    "\n",
    "# compute cost-to-go of initial state via backward recursion\n",
    "for k in range(N_steps):\n",
    "\n",
    "    print(\"Step\", k)\n",
    "\n",
    "    # will contain optimal control input for every state\n",
    "    dp_a = inf*np.ones((N_s1, N_s2))\n",
    "    \n",
    "    # loop through all state combinations\n",
    "    for i1 in range(S1.shape[0]):\n",
    "        for i2 in range(S1.shape[1]):\n",
    "\n",
    "            s_current = np.reshape(np.array([S1[i1, i2], S2[i1, i2]]), (ns, 1))  \n",
    "           \n",
    "            # loop through all control\n",
    "            for j, a_current in enumerate(a_grid):\n",
    "                \n",
    "                # get next state for given current state and control\n",
    "                s_next = s_next_map[i1, i2, j, :]\n",
    "                \n",
    "                # get indices of projected x values\n",
    "                [ix1, ix2] = projected_indices_s1[i1,i2,j, :]\n",
    "                [iy1, iy2] = projected_indices_s2[i1,i2,j, :]\n",
    "                \n",
    "                # if not outside the grid\n",
    "                if not(ix1 < 0 or iy1 < 0 or ix2 >= N_s1 or iy2 >= N_s2):\n",
    "\n",
    "                    J_val = interpolate_bilinear(s_next[0], s_next[1],\n",
    "                             s1_grid[ix1], s2_grid[iy1], s1_grid[ix2], s2_grid[iy2],\n",
    "                             dp_cost_to_go[ix1, iy1, k], dp_cost_to_go[ix1, iy2, k], dp_cost_to_go[ix2, iy1, k], dp_cost_to_go[ix2, iy2, k])\n",
    "\n",
    "                    # cost of this control at this state\n",
    "                    # FILL IN YOUR CODE HERE\n",
    "#                     cost = ...\n",
    "\n",
    "                    if cost < dp_cost_to_go[i1,i2, k+1]:\n",
    "# FILL IN YOUR CODE HERE\n",
    "#                         dp_cost_to_go[i1,i2, k+1] = ...\n",
    "#                         dp_a[i1,i2] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0186627",
   "metadata": {},
   "source": [
    "### 3.3 Comparison LQR and DP\n",
    "\n",
    "Consider the following plots showing the cost of DP and LQR as well as their control policies. Where is the LQR policy similar to the one obtained with DP? Where is it different? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import *\n",
    "\n",
    "# set inf values to nan so they are not shown in plot\n",
    "J_final = dp_cost_to_go[:, :, -1]\n",
    "J_final[J_final == inf] = np.nan\n",
    "\n",
    "plot_control_maps(dp_a, LQR_a, S1, S2, N_steps, amin=a_min, amax=a_max)\n",
    "plot_cost_maps(J_final, LQR_cost, S1, S2, N_steps, log_scale=False)\n",
    "plot_cost_maps(J_final, LQR_cost, S1, S2, N_steps, log_scale=True)\n",
    "\n",
    "plot_indices_s2 = [15, 25, 35]\n",
    "plot_cost_slices(J_final, LQR_cost, s1_grid, s2_grid, plot_indices_s2, vary_s1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36c73d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55be551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370b040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f273b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
